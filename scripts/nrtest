#!/usr/bin/env python

# system imports
import logging
from os import listdir
from os.path import exists, isfile, isdir, join
from argparse import ArgumentParser
from sys import exit

# nrtest imports
from nrtest.testsuite import TestSuite
from nrtest.compare import compare_testsuite


def execute(args):
    for p in args.tests + [args.app]:
        if not exists(p):
            logging.error('Could not find path: "%s"' % p)

    test_dirs = [p for p in args.tests if isdir(p)]
    test_files = [p for p in args.tests if isfile(p)]
    test_files += [join(d, p) for d in test_dirs for p in listdir(d)
                   if p.endswith('.json')]

    test_files = list(set(test_files))  # remove duplicates
    logging.info('Found %i tests' % len(test_files))

    ts = TestSuite.read_config(args.app, test_files)

    try:
        success = ts.execute()
        ts.write_manifest()
    except KeyboardInterrupt:
        logging.warning('Process interrupted by user')
        success = False
    else:
        logging.info('Finished')

    # Non-zero exit code indicates failure
    exit(not success)


def compare(args):
    manifest_new = join(args.new, TestSuite.manifest_fname)
    manifest_old = join(args.old, TestSuite.manifest_fname)

    ts_new = TestSuite.read_manifest(manifest_new)
    ts_old = TestSuite.read_manifest(manifest_old)

    ts_new.dir = args.new
    ts_old.dir = args.old

    compatible = compare_testsuite(ts_new, ts_old, args.tolerance)

    # Non-zero exit code indicates failure
    exit(not compatible)


if __name__ == '__main__':
    parser = ArgumentParser(description='Numerical regression testing')
    parser.add_argument('-q', '--quiet', help='suppress normal messages',
                        dest='LOGLEVEL', action='store_const',
                        const=logging.WARNING, default=logging.INFO)
    parser.add_argument('-v', '--verbose', help='output debug messages',
                        dest='LOGLEVEL', action='store_const',
                        const=logging.DEBUG, default=logging.INFO)
    subparsers = parser.add_subparsers(title='subcommands')

    execute_parser = subparsers.add_parser('execute', help='execute tests')
    execute_parser.set_defaults(func=execute)
    execute_parser.add_argument('app', metavar='app.json',
                                help='application config card')
    execute_parser.add_argument('tests', metavar='test.json', nargs='+',
                                help='test config card or directory containing\
                                such cards')

    compare_parser = subparsers.add_parser('compare', help='compare results')
    compare_parser.set_defaults(func=compare)
    compare_parser.add_argument('new', metavar='new_benchmark')
    compare_parser.add_argument('old', metavar='old_benchmark')
    compare_parser.add_argument('-t', '--tolerance', default=0.01,
                                help='Relative precision at which results \
                                considered compatible')

    args = parser.parse_args()

    LOGFORMAT = '%(levelname)s: %(message)s'
    logging.basicConfig(level=args.LOGLEVEL, format=LOGFORMAT)

    args.func(args)
